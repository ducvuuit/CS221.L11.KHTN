{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pbcquoc/transformer/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2ENKuobJQXp"
      },
      "source": [
        "# Giới Thiệu Transformer\n",
        "Sự nổi tiếng của mô hình Transformer thì không cần phải bàn cãi, vì nó chính là nền tảng của rất nhiều mô hình khác mà nổi tiếng nhất là BERT (Bidirectional Encoder Representations from Transformers) một mô hình dùng để học biểu diễn của các từ tốt nhất hiện tại và đã tạo ra một bước ngoặc lớn cho động đồng NLP trong năm 2019. Và chính Google cũng đã áp dụng BERT trong cỗ máy tìm kiếm của họ. Để hiểu BERT, các bạn cần phải nắm rõ về mô hình Transformer.\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/bert.jpg)\n",
        "\n",
        "Ý tưởng chủ đạo của Transformer vẫn là áp dụng cơ thể Attention, những ở mức phức tạp hơn và thật sự là thú vị hơn so với cách được đề xuất trước đó trong một bài báo của tác giả Lương Minh Thắng, một người Việt rất nổi tiếng trong cộng đồng deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb7MbGxAJnBW"
      },
      "source": [
        "## Tổng Quan Mô Hình\n",
        "Để cho dễ cảm nhận được cách mà mô hình hoạt động, mình sẽ trình bày trước toàn bộ kiến trúc mô hình ở mức high-level và sau đó sẽ đi chi tiết từng phần nhỏ cũng như công thức toán của nó.\n",
        "\n",
        "Giống như những mô hình dịch máy khác, kiến trúc tổng quan của mô hình transformer bao gồm 2 phần lớn là encoder và decoder. Encoder dùng để học vector biểu của câu với mong muốn rằng vector này mang thông tin hoàn hảo của câu đó. Decoder thực hiện chức năng chuyển vector biểu diễn kia thành ngôn ngữ đích.\n",
        "\n",
        "Trong ví dụ ở dưới, encoder của mô hình transformer nhận một câu tiếng việt, và encode thành một vector biểu diễn ngữ nghĩa của câu little sun, sau đó mô hình decoder nhận vector biểu diễn này, và dịch nó thành câu tiếng việt mặt trời bé nhỏ\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/overview.jpg)\n",
        "\n",
        "Một trong những ưu điểm của transformer là mô hình này có khả năng xử lý song song cho các từ. Như các bạn thấy, Encoders của mô hình transfomer là một dạng feedforward neural nets, bao gồm nhiều encoder layer khác, mỗi encoder layer này xử lý đồng thời các từ. Trong khi đó, với mô hình LSTM, thì các từ phải được xử lý tuần tự. Ngoài ra, mô hình Transformer còn xử lý câu đầu vào theo 2 hướng mà không cần phải stack thêm một môt hình LSTM nữa như trong kiến trúc Bidirectional LSTM.\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/overview2.jpg)\n",
        "\n",
        "Một cái nhìn vừa tổng quát và chi tiết sẽ giúp ích cho các bạn. Mình sẽ đi vào chi tiết một số phần cực kì quan trọng như sinusoidal position encoding, multi head attention của encoder, còn của decoder thì các bạn thấy được kiến trúc rất giống với của encoder, do đó mình sẽ chỉ đi nhanh qua mà thôi.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/overview3.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVOKeezWPsSs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "903e857d-34fe-460e-ec1c-b48659b341a6"
      },
      "source": [
        "! pip -q install torchtext==0.6.0\n",
        "! pip -q install pyvi \n",
        "! pip -q install https://github.com/trungtv/vi_spacy/raw/master/packages/vi_spacy_model-0.2.1/dist/vi_spacy_model-0.2.1.tar.gz\n",
        "! python -m spacy link vi_spacy_model vi_spacy_model\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for vi-spacy-model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "\u001b[38;5;1m✘ Link 'vi_spacy_model' already exists\u001b[0m\n",
            "To overwrite an existing link, use the --force flag\n",
            "\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gvN64qvNQIS"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import os\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM1dnT0JKkdk"
      },
      "source": [
        "# Embedding Layer with Position Encoding\n",
        "Trước khi đi vào mô hình encoder, chúng ta sẽ tìm hiểu cơ chế rất thú vị là Position Encoding dùng để đưa thông tin về vị trí của các từ vào mô hình transformer.\n",
        "\n",
        "Đầu tiên, các từ được biểu diễn bằng một vector sử dụng một ma trận word embedding có số dòng bằng kích thước của tập từ vựng. Sau đó các từ trong câu được tìm kiếm trong ma trận này, và được nối nhau thành các dòng của một ma trận 2 chiều chứa ngữ nghĩa của từng từ riêng biệt. Nhưng như các bạn đã thấy, transformer xử lý các từ song song, do đó, với chỉ word embedding mô hình không thể nào biết được vị trí các từ. Như vậy, chúng ta cần một cơ chế nào đó để đưa thông tin vị trí các từ vào trong vector đầu vào. Đó là lúc positional encoding xuất hiện và giải quyết vấn đề của chúng ta. Tuy nhiên, trước khi giới thiệu cơ chế position encoding của tác giả, các bạn có thể giải quyết vấn đề băng một số cách naive như sau:\n",
        "\n",
        "Biểu diễn vị trí các từ bằng chuỗi các số liên tục từ 0,1,2,3 …, n. Tuy nhiên, chúng ta gặp ngay vấn đề là khi chuỗi dài thì số này có thể khá lớn, và mô hình sẽ gặp khó khăn khi dự đoán những câu có chiều dài lớn hơn tất cả các câu có trong tập huấn luyện. Để giải quyết vấn đề này, các bạn có thể chuẩn hóa lại cho chuỗi số này nằm trong đoạn từ 0-1 bằng cách chia cho n nhưng mà chúng ta sẽ gặp vấn đề khác là khoảng cách giữ 2 từ liên tiếp sẽ phụ thuộc vào chiều dài của chuỗi, và trong một khoản cố định, chúng ta không hình dùng được khoản đó chứa bao nhiêu từ. Điều này có nghĩa là ý nghĩa của position encoding sẽ khác nhau tùy thuộc vào độ dài của câu đó."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9da_ZuSNQIW"
      },
      "source": [
        "class Embedder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.embed(x)\n",
        "    \n",
        "# Embedder(100, 512)(torch.LongTensor([1,2,3,4])).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGlFpeIQKss-"
      },
      "source": [
        "# Phương pháp đề xuất sinusoidal position encoding\n",
        "Phương pháp của tác giả đề xuất không gặp những hạn chế mà chúng ta vừa nêu. Vị trí của các từ được mã hóa bằng một vector có kích thước bằng word embedding và được cộng trực tiếp vào word embedding.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/embedding.jpg)\n",
        "\n",
        "Cụ thể, tại vị trí chẵn, tác giả sử dụng hàm sin, và với vị trí lẽ tác giả sử dụng hàm cos để tính giá trị tại chiều đó.\n",
        "\n",
        "![alt text](https://github.com/pbcquoc/pbcquoc.github.io/raw/master/images/transformer/pe_formula.png)\n",
        "\n",
        "Trong hình dưới này, mình minh họa cho cách tính position encoding của tác giả. Giả sử chúng ta có word embedding có 6 chiều, thì position encoding cũng có tương ứng là 6 chiều. Mỗi dòng tương ứng với một từ. Giá trị của các vector tại mỗi vị trí được tính toán theo công thức ở hình dưới.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe.png)\n",
        "\n",
        "Lúc này một số bạn sẽ thắc mắc tại sao với cách biểu diện vị trí như tác giả đề xuất lại có thể mã hóa thông tin vị trí của từ? Hãy tưởng tượng bạn có các số từ 0-15. Các bạn có thể thấy rằng bit ngoài cùng bên phải thay đổi nhanh nhất mỗi 1 số, và sau đó là bit bên phải thứ 2, thay đổi mỗi 2 số, tương tự cho các bit khác.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe_intuition.jpg)\n",
        "\n",
        "Trong công thức của tác giả đề xuất, các bạn cũng thấy rằng, hàm sin và cos có dạng đồ thị tần số và tần số này giảm dần ở các chiều lớn dần. Các bạn xem hình dưới, ở chiều 0, giá trị thay đổi liên tục tương ứng với màu sắc thay đổi liên tục, và tần số thay đổi này giảm dần ở các chiều lớn hơn.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe_heatmap.png)\n",
        "\n",
        "Nên chúng ta có thể cảm nhận được việc biểu diễn của tác giả khá tương tự như cách biểu diễn các số nguyên trong hệ nhị phân, cho nên chúng ta có thể biểu diễn được vị trí các từ theo cách như vậy.\n",
        "\n",
        "Chúng ta cũng có thể xem ma trận khoảng cách của các vector biểu diễn vị trí như hình dưới. Rõ ràng, các vector biểu diễn thể hiện được tính chất khoảng cách giữ 2 từ. 2 từ cách càng xa nhau thì khoảng cách càng lớn hơn.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/pe_distance.png)\n",
        "\n",
        "Ngoài ra, một tính chất của phương pháp tác giả đề xuất là nó cho phép mô hình dễ dàng học được mối quan hệ tương đối giữ các từ. Cụ thể, biểu diễn vị trí của từ t + offset có thể chuyển thành biểu diễn vị trí của từ t bằng một phép biến đổi tuyến tính dựa trên ma trận phép quay.\n",
        "\n",
        "Để dễ hình dung phương pháp của tác giả đề xuất lại hoạt động tốt, các bạn có thể tưởng tượng, hàm sin, và cos, giống như là kim giây và kim phút trên đồng hồ. Với 2 kim này, chúng ta có thể biểu diễn được 3600 vị trí. Và đồng thời có thể hiểu được ngay tại sao biểu diễn của từ t + offset và từ t lại có thể dễ dàng chuyển đổi cho nhau.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP64KizDNQIa"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length=200, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        pe = torch.zeros(max_seq_length, d_model)\n",
        "        \n",
        "        # Bảng pe mình vẽ ở trên \n",
        "        for pos in range(max_seq_length):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe[pos, i] = math.sin(pos/(10000**(2*i/d_model)))\n",
        "                pe[pos, i+1] = math.cos(pos/(10000**((2*i+1)/d_model)))\n",
        "        pe = pe.unsqueeze(0)        \n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = x*math.sqrt(self.d_model)\n",
        "        seq_length = x.size(1)\n",
        "        \n",
        "        pe = Variable(self.pe[:, :seq_length], requires_grad=False)\n",
        "        \n",
        "        if x.is_cuda:\n",
        "            pe.cuda()\n",
        "        # cộng embedding vector với pe \n",
        "        x = x + pe\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "# PositionalEncoder(512)(torch.rand(5, 30, 512)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdEQKhVvNiSz"
      },
      "source": [
        "# Encoder\n",
        "Encoder của mô hình transformer có thể bao gồm nhiều encoder layer tượng tự nhau. Mỗi encoder layer của transformer lại bao gồm 2 thành phần chính là multi head attention và feedforward network, ngoài ra còn có cả skip connection và normalization layer.\n",
        "\n",
        "Trong 2 thành phần chính này, các bạn sẽ hứng thú nhiều hơn về multi-head attention vì đó là một layer mới được giới thiệu trong bài báo này, và chính nó tạo nên sự khác biệt giữ mô hình LSTM và mô hình Transformer mà chúng ta đang tìm hiểu.\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/encoder.jpg)\n",
        "\n",
        "Encoder đầu tiên sẽ nhận ma trận biểu diễn của các từ đã được cộng với thông tin vị trí thông qua positional encoding. Sau đó, ma trận này sẽ được xử lý bởi Multi Head Attention. Multi Head Attention thật chất là self-attention, nhưng mà để mô hình có thể có chú ý nhiều pattern khác nhau, tác giả đơn giản là sử dụng nhiều self-attention.\n",
        "\n",
        "## Self Attention Layer\n",
        "Self Attention cho phép mô hình khi mã hóa một từ có thể sử dụng thông tin của những từ liên quan tới nó. Ví dụ khi từ nó được mã hóa, nó sẽ chú ý vào các từ liên quan như là mặt trời. Cơ chế self attention này có ý nghĩa tương tự như cơ chế attention mình đã chia sẻ ở bài trước và những công thức toán học cũng tương ứng với nhau.\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/self_attention.jpg)\n",
        "\n",
        "Bạn có thể tưởng tượng cơ chế self attention giống như cơ chế tìm kiếm. Với một từ cho trước, cơ chế này sẽ cho phép mô hình tìm kiếm trong cách từ còn lại, từ nào “giống” để sau đó thông tin sẽ được mã hóa dựa trên tất cả các từ trên.\n",
        "\n",
        "Đầu tiên, với môi từ chúng ta cần tạo ra 3 vector: query, key, value vector bằng cách nhân ma trận biểu diễn các từ đầu vào với ma trận học tương ứng.\n",
        "\n",
        "* query vector: vector dùng để chứa thông tin của từ được tìm kiếm, so sách. Giống như là câu query của google search.\n",
        "* key vector: vector dùng để biểu diễn thông tin các từ được so sánh với từ cần tìm kiếm ở trên. Ví dụ, như các trang webs mà google sẽ so sánh với từ khóa mà bạn tìm kiếm.\n",
        "* value vector: vector biểu diễn nội dung, ý nghĩa của các từ. Các bạn có thể tượng tượng, nó như là nội dung trang web được hiển thị cho người dùng sau khi tìm kiếm.\n",
        "Để tính tương quan, chúng ta đơn giản chỉ cần tính tích vô hướng dựa các vector query và key. Sau đó dùng hàm softmax để chuẩn hóa chỉ số tương quan trong đoạn 0-1, và cuối cùng, tính trung bình cộng có trọng số giữa các vector values sử dụng chỉ số tương quan mới tính được. Quá dễ !!!\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/self_attention_2.png)\n",
        "\n",
        "Cụ thể hơn, quá trình tính toán attention vector có thể được tóm tắt làm 3 bước như sau:\n",
        "\n",
        "* Bước 1: Tính ma trận query, key, value bằng cách khởi tạo 3 ma trận trọng số query, key, vector. Sau đó nhân input với các ma trận trọng số này để tạo thành 3 ma trận tương ứng.\n",
        "* Bước 2: Tính attention weights. Nhân 2 ma trận key, query vừa được tính ở trên với nhau để với ý nghĩa là so sánh giữ câu query và key để học mối tương quan. Sau đó thì chuẩn hóa về đoạn [0-1] bằng hàm softmax. 1 có nghĩa là câu query giống với key, 0 có nghĩa là không giống.\n",
        "* Bước 3: Tính output. Nhân attention weights với ma trận value. Điều này có nghĩa là chúng ta biểu diễn một từ bằng trung bình có trọng số (attention weights) của ma trận value.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/attention_vector.jpg)\n",
        "\n",
        "## Multi Head Attention\n",
        "Chúng ta muốn mô hình có thể học nhiều kiểu mối quan hệ giữ các từ với nhau. Với mỗi self-attention, chúng ta học được một kiểu pattern, do đó để có thể mở rộng khả năng này, chúng ta đơn giản là thêm nhiều self-attention. Tức là chúng ta cần nhiều ma trận query, key, value mà thôi. Giờ đây ma trận trọng số key, query, value sẽ có thêm 1 chiều depth nữa.\n",
        "\n",
        "![](https://pbcquoc.github.io/images/transformer/multi_head_attention.jpg)\n",
        "\n",
        "Multi head attention cho phép mô hình chú ý đến đồng thời những pattern dễ quan sát được như sau.\n",
        "\n",
        "* Chú ý đến từ kế trước của một từ\n",
        "* Chú ý đến từ kế sau của một từ\n",
        "* Chú ý đến những từ liên quan của một từ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJMcGuUNQId"
      },
      "source": [
        "def attention(q, k, v, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    q: batch_size x head x seq_length x d_model\n",
        "    k: batch_size x head x seq_length x d_model\n",
        "    v: batch_size x head x seq_length x d_model\n",
        "    mask: batch_size x 1 x 1 x seq_length\n",
        "    output: batch_size x head x seq_length x d_model\n",
        "    \"\"\"\n",
        "\n",
        "    # attention score được tính bằng cách nhân q với k\n",
        "    d_k = q.size(-1)\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(d_k)\n",
        "    \n",
        "    if mask is not None:\n",
        "        mask = mask.unsqueeze(1)\n",
        "        scores = scores.masked_fill(mask==0, -1e9)\n",
        "    # xong rồi thì chuẩn hóa bằng softmax\n",
        "    scores = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        scores = dropout(scores)\n",
        "    \n",
        "    output = torch.matmul(scores, v)\n",
        "    return output, scores\n",
        "\n",
        "# attention(torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512), torch.rand(32, 8, 30, 512)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQ4C3EENQIh"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.d_k = d_model//heads\n",
        "        self.h = heads\n",
        "        self.attn = None\n",
        "\n",
        "        # tạo ra 3 ma trận trọng số là q_linear, k_linear, v_linear như hình trên\n",
        "        self.q_linear = nn.Linear(d_model, d_model)\n",
        "        self.k_linear = nn.Linear(d_model, d_model)\n",
        "        self.v_linear = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.out = nn.Linear(d_model, d_model)\n",
        "    \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        \"\"\"\n",
        "        q: batch_size x seq_length x d_model\n",
        "        k: batch_size x seq_length x d_model\n",
        "        v: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        bs = q.size(0)\n",
        "        # nhân ma trận trọng số q_linear, k_linear, v_linear với dữ liệu đầu vào q, k, v \n",
        "        # ở bước encode các bạn lưu ý rằng q, k, v chỉ là một (xem hình trên)\n",
        "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
        "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
        "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
        "        \n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        \n",
        "        # tính attention score\n",
        "        scores, self.attn = attention(q, k, v, mask, self.dropout)\n",
        "        \n",
        "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
        "        \n",
        "        output = self.out(concat)\n",
        "        return output\n",
        "\n",
        "# MultiHeadAttention(8, 512)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 30, 512)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvOrq4-WPXYK"
      },
      "source": [
        "# Residuals Connection và Normalization Layer\n",
        "\n",
        "Trong kiến trúc của mô hình transformer, residuals connection và mormalization layer được sử dụng mọi nơi, giống như tinh thần của nó. 2 kỹ thuật giúp cho mô hình huấn luyện nhanh hội tụ hơn và trách mất mát thông tin trong quá trình huấn luyện mô hình, ví dụ như là thông tin của vị trí các từ được mã hóa.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-_9Hq-NQIk"
      },
      "source": [
        "class Norm(nn.Module):\n",
        "    def __init__(self, d_model, eps = 1e-6):\n",
        "        super().__init__()\n",
        "    \n",
        "        self.size = d_model\n",
        "        \n",
        "        # create two learnable parameters to calibrate normalisation\n",
        "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
        "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
        "        \n",
        "        self.eps = eps\n",
        "    \n",
        "    def forward(self, x):\n",
        "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
        "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
        "        return norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1ndbdMXNQIn"
      },
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" Trong kiến trúc của chúng ta có tầng linear \n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
        "        super().__init__() \n",
        "    \n",
        "        # We set d_ff as a default to 2048\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wwo91xDNQIq"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.attn = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        \n",
        "        \n",
        "        x2 = self.norm_1(x)\n",
        "        # tính attention value, các bạn để ý q, k, v là giống nhau        \n",
        "        x = x + self.dropout_1(self.attn(x2,x2,x2,mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        x = x + self.dropout_2(self.ff(x2))\n",
        "        return x\n",
        "\n",
        "# EncoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32 , 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48SiXL_zQQ9C"
      },
      "source": [
        "# Decoder\n",
        "Decoder thực hiện chức năng giải mã vector của câu nguồn thành câu đích, do đó decoder sẽ nhận thông tin từ encoder là 2 vector key và value. Kiến trúc của decoder rất giống với encoder, ngoại trừ có thêm một multi head attention nằm ở giữ dùng để học mối liên quan giữ từ đang được dịch với các từ được ở câu nguồn.\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/decoder.jpg)\n",
        "\n",
        "## Masked Multi Head Attention\n",
        "Masked Multi Head Attention tất nhiên là multi head attention mà chúng ta đã nói đến ở trên, có chức năng dùng đề encode các từ câu câu đích trong quá trình dịch, tuy nhiên, lúc cài đặt chúng ta cần lưu ý rằng phải che đi các từ ở tương lai chưa được mô hình dịch đến, để làm việc này thì đơn giản là chúng ta chỉ cần nhân với một vector chứa các giá trị 0,1.\n",
        "\n",
        "Trong decoder còn có một multi head attention khác có chức năng chú ý các từ ở mô hình encoder, layer này nhận vector key và value từ mô hình encoder, và output từ layer phía dưới. Đơn giản bởi vì chúng ta muốn so sánh sự tương quan giữ từ đang được dịch vời các từ nguồn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mDt2NPeNQIu"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = Norm(d_model)\n",
        "        self.norm_2 = Norm(d_model)\n",
        "        self.norm_3 = Norm(d_model)\n",
        "        \n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "        self.dropout_3 = nn.Dropout(dropout)\n",
        "        \n",
        "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.attn_2 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
        "        self.ff = FeedForward(d_model, dropout=dropout)\n",
        "\n",
        "    def forward(self, x, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        x: batch_size x seq_length x d_model\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        \"\"\"\n",
        "        # Các bạn xem hình trên, kiến trúc mình vẽ với code ở chỗ này tương đương nhau.\n",
        "        x2 = self.norm_1(x)\n",
        "        # multihead attention thứ nhất, chú ý các từ ở target \n",
        "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
        "        x2 = self.norm_2(x)\n",
        "        # masked mulithead attention thứ 2. k, v là giá trị output của mô hình encoder\n",
        "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs, src_mask))\n",
        "        x2 = self.norm_3(x)\n",
        "        x = x + self.dropout_3(self.ff(x2))\n",
        "        return x\n",
        "    \n",
        "# DecoderLayer(512, 8)(torch.rand(32, 30, 512), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk1c6NkYIeG8"
      },
      "source": [
        "# Cài đặt Encoder\n",
        "bao gồm N encoder layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcU8nyvzNQIx"
      },
      "source": [
        "import copy\n",
        "\n",
        "def get_clones(module, N):\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"Một encoder có nhiều encoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(EncoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "        \n",
        "    def forward(self, src, mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Encoder(232, 512,6,8,0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qip-E_TAIpmJ"
      },
      "source": [
        "# Cài đặt Decoder\n",
        "bao gồm N decoder layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lBRYMg_NQI0"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"Một decoder có nhiều decoder layer nhé !!!\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.N = N\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
        "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
        "        self.norm = Norm(d_model)\n",
        "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        trg: batch_size x seq_length\n",
        "        e_outputs: batch_size x seq_length x d_model\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask: batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x d_model\n",
        "        \"\"\"\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for i in range(self.N):\n",
        "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
        "        return self.norm(x)\n",
        "    \n",
        "# Decoder(232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.rand(32, 30, 512), torch.rand(32, 1, 30), torch.rand(32, 1, 30)).shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDVQGAaMI5UU"
      },
      "source": [
        "# Cài đặt Transformer \n",
        "bao gồm encoder và decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpxSCRILNQI3"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\" Cuối cùng ghép chúng lại với nhau để được mô hình transformer hoàn chỉnh\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout)\n",
        "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
        "        self.out = nn.Linear(d_model, trg_vocab)\n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        \"\"\"\n",
        "        src: batch_size x seq_length\n",
        "        trg: batch_size x seq_length\n",
        "        src_mask: batch_size x 1 x seq_length\n",
        "        trg_mask batch_size x 1 x seq_length\n",
        "        output: batch_size x seq_length x vocab_size\n",
        "        \"\"\"\n",
        "        e_outputs = self.encoder(src, src_mask)\n",
        "        \n",
        "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
        "        output = self.out(d_output)\n",
        "        return output\n",
        "    \n",
        "# Transformer(232, 232, 512, 6, 8, 0.1)(torch.LongTensor(32, 30).random_(0, 10), torch.LongTensor(32, 30).random_(0, 10),torch.rand(32, 1, 30),torch.rand(32, 1, 30)).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVj-ECLzHLxf"
      },
      "source": [
        "Chúng ta sử dụng torchtext để load dữ liệu, giúp giảm thời gian và hiệu quả "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5tvzW9jNQI6"
      },
      "source": [
        "from torchtext import data\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkBjLH96NQI8"
      },
      "source": [
        "\n",
        "def nopeak_mask(size, device):\n",
        "    \"\"\"Tạo mask được sử dụng trong decoder để lúc dự đoán trong quá trình huấn luyện\n",
        "     mô hình không nhìn thấy được các từ ở tương lai\n",
        "    \"\"\"\n",
        "    np_mask = np.triu(np.ones((1, size, size)),\n",
        "    k=1).astype('uint8')\n",
        "    np_mask =  Variable(torch.from_numpy(np_mask) == 0)\n",
        "    np_mask = np_mask.to(device)\n",
        "    \n",
        "    return np_mask\n",
        "\n",
        "def create_masks(src, trg, src_pad, trg_pad, device):\n",
        "    \"\"\" Tạo mask cho encoder, \n",
        "    để mô hình không bỏ qua thông tin của các kí tự PAD do chúng ta thêm vào \n",
        "    \"\"\"\n",
        "    src_mask = (src != src_pad).unsqueeze(-2)\n",
        "\n",
        "    if trg is not None:\n",
        "        trg_mask = (trg != trg_pad).unsqueeze(-2)\n",
        "        size = trg.size(1) # get seq_len for matrix\n",
        "        np_mask = nopeak_mask(size, device)\n",
        "        if trg.is_cuda:\n",
        "            np_mask.cuda()\n",
        "        trg_mask = trg_mask & np_mask\n",
        "        \n",
        "    else:\n",
        "        trg_mask = None\n",
        "    return src_mask, trg_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YoUVx4xjEb7"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "import re\n",
        "\n",
        "def get_synonym(word, SRC):\n",
        "    syns = wordnet.synsets(word)\n",
        "    for s in syns:\n",
        "        for l in s.lemmas():\n",
        "            if SRC.vocab.stoi[l.name()] != 0:\n",
        "                return SRC.vocab.stoi[l.name()]\n",
        "            \n",
        "    return 0\n",
        "\n",
        "def multiple_replace(dict, text):\n",
        "  # Create a regular expression  from the dictionary keys\n",
        "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
        "\n",
        "  # For each match, look-up corresponding value in dictionary\n",
        "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IJpUEIMgMbw"
      },
      "source": [
        "def init_vars(src, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\" Tính toán các ma trận cần thiết trong quá trình translation sau khi mô hình học xong\n",
        "    \"\"\"\n",
        "    init_tok = TRG.vocab.stoi['<sos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "\n",
        "    # tính sẵn output của encoder \n",
        "    e_output = model.encoder(src, src_mask)\n",
        "    \n",
        "    outputs = torch.LongTensor([[init_tok]])\n",
        "    \n",
        "    outputs = outputs.to(device)\n",
        "    \n",
        "    trg_mask = nopeak_mask(1, device)\n",
        "    # dự đoán kí tự đầu tiên\n",
        "    out = model.out(model.decoder(outputs,\n",
        "    e_output, src_mask, trg_mask))\n",
        "    out = F.softmax(out, dim=-1)\n",
        "    \n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_scores = torch.Tensor([math.log(prob) for prob in probs.data[0]]).unsqueeze(0)\n",
        "    \n",
        "    outputs = torch.zeros(k, max_len).long()\n",
        "    outputs = outputs.to(device)\n",
        "    outputs[:, 0] = init_tok\n",
        "    outputs[:, 1] = ix[0]\n",
        "    \n",
        "    e_outputs = torch.zeros(k, e_output.size(-2),e_output.size(-1))\n",
        "   \n",
        "    e_outputs = e_outputs.to(device)\n",
        "    e_outputs[:, :] = e_output[0]\n",
        "    \n",
        "    return outputs, e_outputs, log_scores\n",
        "\n",
        "def k_best_outputs(outputs, out, log_scores, i, k):\n",
        "    \n",
        "    probs, ix = out[:, -1].data.topk(k)\n",
        "    log_probs = torch.Tensor([math.log(p) for p in probs.data.view(-1)]).view(k, -1) + log_scores.transpose(0,1)\n",
        "    k_probs, k_ix = log_probs.view(-1).topk(k)\n",
        "    \n",
        "    row = k_ix // k\n",
        "    col = k_ix % k\n",
        "\n",
        "    outputs[:, :i] = outputs[row, :i]\n",
        "    outputs[:, i] = ix[row, col]\n",
        "\n",
        "    log_scores = k_probs.unsqueeze(0)\n",
        "    \n",
        "    return outputs, log_scores\n",
        "\n",
        "def beam_search(src, model, SRC, TRG, device, k, max_len):    \n",
        "\n",
        "    outputs, e_outputs, log_scores = init_vars(src, model, SRC, TRG, device, k, max_len)\n",
        "    eos_tok = TRG.vocab.stoi['<eos>']\n",
        "    src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(-2)\n",
        "    ind = None\n",
        "    for i in range(2, max_len):\n",
        "    \n",
        "        trg_mask = nopeak_mask(i, device)\n",
        "\n",
        "        out = model.out(model.decoder(outputs[:,:i],\n",
        "        e_outputs, src_mask, trg_mask))\n",
        "\n",
        "        out = F.softmax(out, dim=-1)\n",
        "    \n",
        "        outputs, log_scores = k_best_outputs(outputs, out, log_scores, i, k)\n",
        "        \n",
        "        ones = (outputs==eos_tok).nonzero() # Occurrences of end symbols for all input sentences.\n",
        "        sentence_lengths = torch.zeros(len(outputs), dtype=torch.long).cuda()\n",
        "        for vec in ones:\n",
        "            i = vec[0]\n",
        "            if sentence_lengths[i]==0: # First end symbol has not been found yet\n",
        "                sentence_lengths[i] = vec[1] # Position of first end symbol\n",
        "\n",
        "        num_finished_sentences = len([s for s in sentence_lengths if s > 0])\n",
        "\n",
        "        if num_finished_sentences == k:\n",
        "            alpha = 0.7\n",
        "            div = 1/(sentence_lengths.type_as(log_scores)**alpha)\n",
        "            _, ind = torch.max(log_scores * div, 1)\n",
        "            ind = ind.data[0]\n",
        "            break\n",
        "    \n",
        "    if ind is None:\n",
        "        \n",
        "        length = (outputs[0]==eos_tok).nonzero()[0] if len((outputs[0]==eos_tok).nonzero()) > 0 else -1\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[0][1:length]])\n",
        "    \n",
        "    else:\n",
        "        length = (outputs[ind]==eos_tok).nonzero()[0]\n",
        "        return ' '.join([TRG.vocab.itos[tok] for tok in outputs[ind][1:length]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-AFuSOIhi7X"
      },
      "source": [
        "def translate_sentence(sentence, model, SRC, TRG, device, k, max_len):\n",
        "    \"\"\"Dịch một câu sử dụng beamsearch\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    indexed = []\n",
        "    sentence = SRC.preprocess(sentence)\n",
        "    \n",
        "    for tok in sentence:\n",
        "        if SRC.vocab.stoi[tok] != SRC.vocab.stoi['<eos>']:\n",
        "            indexed.append(SRC.vocab.stoi[tok])\n",
        "        else:\n",
        "            indexed.append(get_synonym(tok, SRC))\n",
        "    \n",
        "    sentence = Variable(torch.LongTensor([indexed]))\n",
        "    \n",
        "    sentence = sentence.to(device)\n",
        "    \n",
        "    sentence = beam_search(sentence, model, SRC, TRG, device, k, max_len)\n",
        "\n",
        "    return  multiple_replace({' ?' : '?',' !':'!',' .':'.','\\' ':'\\'',' ,':','}, sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uee4YaQNQI_"
      },
      "source": [
        "import spacy\n",
        "import re\n",
        "\n",
        "class tokenize(object):\n",
        "    \n",
        "    def __init__(self, lang):\n",
        "        self.nlp = spacy.load(lang)\n",
        "            \n",
        "    def tokenizer(self, sentence):\n",
        "        sentence = re.sub(\n",
        "        r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", str(sentence))\n",
        "        sentence = re.sub(r\"[ ]+\", \" \", sentence)\n",
        "        sentence = re.sub(r\"\\!+\", \"!\", sentence)\n",
        "        sentence = re.sub(r\"\\,+\", \",\", sentence)\n",
        "        sentence = re.sub(r\"\\?+\", \"?\", sentence)\n",
        "        sentence = sentence.lower()\n",
        "        return [tok.text for tok in self.nlp.tokenizer(sentence) if tok.text != \" \"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2jMF9lzQ4a8"
      },
      "source": [
        "## Data loader\n",
        "Sử dụng torchtext để load dữ liệu nhanh chóng"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uVO0yr_NQJC"
      },
      "source": [
        "import os\n",
        "import dill as pickle\n",
        "import pandas as pd\n",
        "\n",
        "def read_data(src_file, trg_file):\n",
        "    src_data = open(src_file).read().strip().split('\\n')\n",
        "\n",
        "    trg_data = open(trg_file).read().strip().split('\\n')\n",
        " \n",
        "    return src_data, trg_data\n",
        "\n",
        "def create_fields(src_lang, trg_lang):\n",
        "    \n",
        "    print(\"loading spacy tokenizers...\")\n",
        "    \n",
        "    t_src = tokenize(src_lang)\n",
        "    t_trg = tokenize(trg_lang)\n",
        "\n",
        "    TRG = data.Field(lower=True, tokenize=t_trg.tokenizer, init_token='<sos>', eos_token='<eos>')\n",
        "    SRC = data.Field(lower=True, tokenize=t_src.tokenizer)\n",
        "        \n",
        "    return SRC, TRG\n",
        "\n",
        "def create_dataset(src_data, trg_data, max_strlen, batchsize, device, SRC, TRG, istrain=True):\n",
        "\n",
        "    print(\"creating dataset and iterator... \")\n",
        "\n",
        "    raw_data = {'src' : [line for line in src_data], 'trg': [line for line in trg_data]}\n",
        "    df = pd.DataFrame(raw_data, columns=[\"src\", \"trg\"])\n",
        "    \n",
        "    mask = (df['src'].str.count(' ') < max_strlen) & (df['trg'].str.count(' ') < max_strlen)\n",
        "    df = df.loc[mask]\n",
        "\n",
        "    df.to_csv(\"translate_transformer_temp.csv\", index=False)\n",
        "    \n",
        "    data_fields = [('src', SRC), ('trg', TRG)]\n",
        "    train = data.TabularDataset('./translate_transformer_temp.csv', format='csv', fields=data_fields)\n",
        "\n",
        "    train_iter = MyIterator(train, batch_size=batchsize, device=device,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=istrain, shuffle=True)\n",
        "    \n",
        "    os.remove('translate_transformer_temp.csv')\n",
        "    \n",
        "    if istrain:\n",
        "        SRC.build_vocab(train)\n",
        "        TRG.build_vocab(train)\n",
        "\n",
        "    return train_iter\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4POJRxdNQJF"
      },
      "source": [
        "def step(model, optimizer,batch, criterion):\n",
        "    \"\"\"\n",
        "    Một lần cập nhật mô hình\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    \n",
        "    src = batch.src.transpose(0,1).cuda()\n",
        "    trg = batch.trg.transpose(0,1).cuda()\n",
        "    trg_input = trg[:, :-1]\n",
        "    src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "    preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "    ys = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "    loss.backward()\n",
        "    optimizer.step_and_update_lr()\n",
        "    \n",
        "    loss = loss.item()\n",
        "    \n",
        "    return loss    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5sPA-k_NQJI"
      },
      "source": [
        "def validiate(model, valid_iter, criterion):\n",
        "    \"\"\" Tính loss trên tập validation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        total_loss = []\n",
        "        for batch in valid_iter:\n",
        "            src = batch.src.transpose(0,1).cuda()\n",
        "            trg = batch.trg.transpose(0,1).cuda()\n",
        "            trg_input = trg[:, :-1]\n",
        "            src_mask, trg_mask = create_masks(src, trg_input, src_pad, trg_pad, opt['device'])\n",
        "            preds = model(src, trg_input, src_mask, trg_mask)\n",
        "\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            loss = criterion(preds.view(-1, preds.size(-1)), ys)\n",
        "            \n",
        "            loss = loss.item()\n",
        "            \n",
        "            total_loss.append(loss)\n",
        "        \n",
        "    avg_loss = np.mean(total_loss)\n",
        "    \n",
        "    return avg_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QX4b6LLN47qC"
      },
      "source": [
        "# Optimizer\n",
        "Để huấn luyện mô hình transformer, các bạn vẫn sử dụng Adam, tuy nhiên, learning rate cần phải được điều chỉnh trong suốt quá trình học theo công thức sau\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/pbcquoc/pbcquoc.github.io/master/images/transformer/optimizer.png)\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/opt.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW8pRq91rwJR"
      },
      "source": [
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "    def state_dict(self):\n",
        "        optimizer_state_dict = {\n",
        "            'init_lr':self.init_lr,\n",
        "            'd_model':self.d_model,\n",
        "            'n_warmup_steps':self.n_warmup_steps,\n",
        "            'n_steps':self.n_steps,\n",
        "            '_optimizer':self._optimizer.state_dict(),\n",
        "        }\n",
        "        \n",
        "        return optimizer_state_dict\n",
        "    \n",
        "    def load_state_dict(self, state_dict):\n",
        "        self.init_lr = state_dict['init_lr']\n",
        "        self.d_model = state_dict['d_model']\n",
        "        self.n_warmup_steps = state_dict['n_warmup_steps']\n",
        "        self.n_steps = state_dict['n_steps']\n",
        "        \n",
        "        self._optimizer.load_state_dict(state_dict['_optimizer'])\n",
        "        \n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKJoYats5LYn"
      },
      "source": [
        "# Label Smoothing\n",
        "Với mô hình nhiều triệu tham số của transformer, thì việt overfit là chuyện dễ dàng xảy ra. Để hạn chế hiện tượng overfit, các bạn có thể sử dụng kỹ thuật label smoothing. Về cơ bản thì ý tưởng của kỹ thuật này khá đơn giản, chúng ta sẽ phạt mô hình khi nó quá tự tin vào việc dự đoán của mình. Thay vì mã hóa nhãn là một one-hot vector, các bạn sẽ thay đổi nhãn này một chút bằng cách phân bố một tí xác suất vào các trường hợp còn lại.\n",
        "\n",
        "![alt text](https://pbcquoc.github.io/images/transformer/label_smoothing.jpg)\n",
        "\n",
        "Giờ thì các bạn sẽ an tâm khi có thể để số epoch lớn mà không lo rằng mô hình sẽ overfit nặng nề.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHGeSHThtlj-"
      },
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, padding_idx, smoothing=0.0, dim=-1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.dim = dim\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = pred.log_softmax(dim=self.dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(self.smoothing / (self.cls - 2))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "            true_dist[:, self.padding_idx] = 0\n",
        "            mask = torch.nonzero(target.data == self.padding_idx, as_tuple=False)\n",
        "            if mask.dim() > 0:\n",
        "                true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "            \n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg257Gk_Kzzw"
      },
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def bleu(valid_src_data, valid_trg_data, model, SRC, TRG, device, k, max_strlen):\n",
        "    pred_sents = []\n",
        "    for sentence in valid_src_data:\n",
        "        pred_trg = translate_sentence(sentence, model, SRC, TRG, device, k, max_strlen)\n",
        "        pred_sents.append(pred_trg)\n",
        "    \n",
        "    pred_sents = [TRG.preprocess(sent) for sent in pred_sents]\n",
        "    trg_sents = [[sent.split()] for sent in valid_trg_data]\n",
        "    \n",
        "    return bleu_score(pred_sents, trg_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpAl45fgwrW-",
        "outputId": "701496f6-9087-4f03-f093-3810a3f42d14"
      },
      "source": [
        "!unzip /content/en_vi.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/en_vi.zip\n",
            "replace data/tst2013.en? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace data/tst2013.en? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/tst2013.en         \n",
            "replace data/tst2012.vi? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/tst2012.vi         \n",
            "replace data/train.en? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/train.en           \n",
            "replace data/tst2013.vi? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/tst2013.vi         \n",
            "replace data/train.vi? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/train.vi           \n",
            "replace data/tst2012.en? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: data/tst2012.en         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhgu-SPTNQJL"
      },
      "source": [
        "opt = {\n",
        "    'train_src_data':'./data/train.en',\n",
        "    'train_trg_data':'./data/train.vi',\n",
        "    'valid_src_data':'./data/tst2013.en',\n",
        "    'valid_trg_data':'./data/tst2013.vi',\n",
        "    'src_lang':'en',\n",
        "    'trg_lang':'en',#'vi_spacy_model',\n",
        "    'max_strlen':160,\n",
        "    'batchsize':1500,\n",
        "    'device':'cuda',\n",
        "    'd_model': 512,\n",
        "    'n_layers': 6,\n",
        "    'heads': 8,\n",
        "    'dropout': 0.1,\n",
        "    'lr':0.0001,\n",
        "    'epochs':150,\n",
        "    'printevery': 200,\n",
        "    'k':5,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HOay5MrNQJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca2b72b5-aa62-4590-ca02-c944b9d4f5ba"
      },
      "source": [
        "os.makedirs('./data/', exist_ok=True)\n",
        "! gdown --id 1Fuo_ALIFKlUvOPbK5rUA5OfAS2wKn_95"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Fuo_ALIFKlUvOPbK5rUA5OfAS2wKn_95\n",
            "To: /content/en_vi.zip\n",
            "\r0.00B [00:00, ?B/s]\r8.39MB [00:00, 83.0MB/s]\r10.1MB [00:00, 61.6MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIWJTjdLNQJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6c4886-ef3c-4a85-8ea7-d2365fb578d5"
      },
      "source": [
        "! unzip -o en_vi.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  en_vi.zip\n",
            "  inflating: data/tst2013.en         \n",
            "  inflating: data/tst2012.vi         \n",
            "  inflating: data/train.en           \n",
            "  inflating: data/tst2013.vi         \n",
            "  inflating: data/train.vi           \n",
            "  inflating: data/tst2012.en         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBotIB8pNQJU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f76b29ec-89c9-4bac-d66e-9939b31543f1"
      },
      "source": [
        "train_src_data, train_trg_data = read_data(opt['train_src_data'], opt['train_trg_data'])\n",
        "valid_src_data, valid_trg_data = read_data(opt['valid_src_data'], opt['valid_trg_data'])\n",
        "\n",
        "SRC, TRG = create_fields(opt['src_lang'], opt['trg_lang'])\n",
        "train_iter = create_dataset(train_src_data, train_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=True)\n",
        "valid_iter = create_dataset(valid_src_data, valid_trg_data, opt['max_strlen'], opt['batchsize'], opt['device'], SRC, TRG, istrain=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading spacy tokenizers...\n",
            "creating dataset and iterator... \n",
            "creating dataset and iterator... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnw9xrJeNQJX"
      },
      "source": [
        "src_pad = SRC.vocab.stoi['<pad>']\n",
        "trg_pad = TRG.vocab.stoi['<pad>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RccNL8VNQJd"
      },
      "source": [
        "model = Transformer(len(SRC.vocab), len(TRG.vocab), opt['d_model'], opt['n_layers'], opt['heads'], opt['dropout'])\n",
        "\n",
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "model = model.to(opt['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12debLGiNQJg"
      },
      "source": [
        "\n",
        "optimizer = ScheduledOptim(\n",
        "        torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        0.2, opt['d_model'], 4000)\n",
        "\n",
        "criterion = LabelSmoothingLoss(len(TRG.vocab), padding_idx=trg_pad, smoothing=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuqS-kD90jbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38ae2358-28dd-4fe6-cd4f-6e887836e155"
      },
      "source": [
        "! gdown --id 1Ty1bGrd0sCwEqXhsoViCUaNKa3lFwmPH\n",
        "model.load_state_dict(torch.load('./transformer.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Ty1bGrd0sCwEqXhsoViCUaNKa3lFwmPH\n",
            "To: /content/transformer.pth\n",
            "348MB [00:05, 68.5MB/s]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeZqfQPANQJl"
      },
      "source": [
        "# import time\n",
        "\n",
        "# for epoch in range(opt['epochs']):\n",
        "#     total_loss = 0\n",
        "    \n",
        "#     for i, batch in enumerate(train_iter): \n",
        "#         s = time.time()\n",
        "#         loss = step(model, optimizer, batch, criterion)\n",
        "        \n",
        "#         total_loss += loss\n",
        "        \n",
        "#         if (i + 1) % opt['printevery'] == 0:\n",
        "#             avg_loss = total_loss/opt['printevery']\n",
        "#             print('epoch: {:03d} - iter: {:05d} - train loss: {:.4f} - time: {:.4f}'.format(epoch, i, avg_loss, time.time()- s))\n",
        "#             total_loss = 0\n",
        "\n",
        "#     s = time.time()\n",
        "#     valid_loss = validiate(model, valid_iter, criterion)\n",
        "#     bleuscore = bleu(valid_src_data[:500], valid_trg_data[:500], model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "#     print('epoch: {:03d} - iter: {:05d} - valid loss: {:.4f} - bleu score: {:.4f} - time: {:.4f}'.format(epoch, i, valid_loss, bleuscore, time.time() - s))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5E8G0-8QFbj"
      },
      "source": [
        "# bleu(valid_src_data, valid_trg_data, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CwtdJeUNQJo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "69ab20f5-51d2-4f73-9ec6-baaa635a6997"
      },
      "source": [
        "sentence='hello'\n",
        "trans_sent = translate_sentence(sentence, model, SRC, TRG, opt['device'], opt['k'], opt['max_strlen'])\n",
        "trans_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'xin chào'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7UYpmmqj31m"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubjbXZIbjn2l"
      },
      "source": [
        "Nếu các bạn không muốn train tốn thời gian thì có thể sử dụng uncomment để download model mình đã train nhé."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHeTea84_bDP"
      },
      "source": [
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def draw(data, x, y, ax):\n",
        "    seaborn.heatmap(data, \n",
        "                    xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0, \n",
        "                    cbar=False, ax=ax, annot=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izoCgA3zNR2i"
      },
      "source": [
        "# Visualize Encoder\n",
        "Dùng heatmap để visualize giá trị attention sẽ cho chúng ta biết khi encode một câu mô hình chú ý từ gì ở lân cận"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV57CMAZ9XR8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a4ab1f5-3da0-4bbd-a8bf-7f1bde3d46a1"
      },
      "source": [
        "sent = SRC.preprocess(sentence)\n",
        "\n",
        "for layer in range(1, 6, 2):\n",
        "    fig, axs = plt.subplots(1,4, figsize=(30, 15))\n",
        "    print(\"Encoder Layer\", layer+1)\n",
        "    for h in range(4):\n",
        "        draw(model.encoder.layers[layer].attn.attn[0, h].data.cpu(), \n",
        "            sent, sent if h ==0 else [], ax=axs[h])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder Layer 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x1080 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Encoder Layer 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x1080 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Encoder Layer 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x1080 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M4_pHeqNkr1"
      },
      "source": [
        "# Visualize Decoder\n",
        "Ở decoder, các bạn có 2 loại visualization\n",
        "* self attention: giá trị attention khi mô hình decoder mã hóa câu đích lúc dịch\n",
        "* src attention: giá trị attention khi mô hình decoder sử dụng câu src"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miQbM9X9-FD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b866aec-1a22-4f91-a331-4736054e0b80"
      },
      "source": [
        "trg_sent = ['<sos>'] + TRG.preprocess(trans_sent)\n",
        "\n",
        "for layer in range(1, 6, 2):\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
        "    print(\"Decoder Self Layer\", layer+1)\n",
        "    for h in range(4):\n",
        "        draw(model.decoder.layers[layer].attn_1.attn[0, h].data[:len(trg_sent), :len(trg_sent)].cpu(), \n",
        "            trg_sent, trg_sent if h ==0 else [], ax=axs[h])\n",
        "    plt.show()\n",
        "    print(\"Decoder Src Layer\", layer+1)\n",
        "    fig, axs = plt.subplots(1,4, figsize=(20, 10))\n",
        "    for h in range(4):\n",
        "        draw(model.decoder.layers[layer].attn_2.attn[0, h].data[:len(trg_sent), :len(sent)].cpu(), \n",
        "            sent, trg_sent if h ==0 else [], ax=axs[h])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder Self Layer 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Decoder Src Layer 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Decoder Self Layer 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Decoder Src Layer 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Decoder Self Layer 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Decoder Src Layer 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_-5cGJYKiUq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}